{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlhsFMmd3srm"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import math\n",
    "import operator\n",
    "import random\n",
    "from queue import PriorityQueue\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbrHBhZk3srr"
   },
   "outputs": [],
   "source": [
    " %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fn5pmUj_3sru"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PT74uZna3srx"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USc2l4703sr0"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "su-mpVf23sr3"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "e7WfUv_a3sr5",
    "outputId": "8036e117-7a14-4954-93d5-992e56faa23c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 13067 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5171\n",
      "eng 3389\n",
      "['elle est juste decorative .', 'she is just a wallflower .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJzJptxI3sr9"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CG88Cegy3ssA"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, n_layers = 1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, n_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAJ_V5jx3ssC"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers = 1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers  # 1\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7tPxIIf3ssG"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHkSRibI3ssJ"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V42MCqnY3ssM"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djfbGUIw3ssO"
   },
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            loss_values.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hx5EiVJu3ssP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQALs8yP3ssR"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "#         decoder_input = torch.tensor([[SOS_token]])  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "#         beam_width = 3\n",
    "        beam_width = 60\n",
    "        topk = 1  # how many sentence do you want to generate\n",
    "        decoded_batch = []\n",
    "\n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([SOS_token])\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "        number_required = 1\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "        nodes = PriorityQueue()\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        qsize = 1\n",
    "\n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 2000: \n",
    "                break\n",
    "#             print(nodes.get())\n",
    "            # fetch the best node\n",
    "            score, n = nodes.get()\n",
    "            \n",
    "            # print('--best node seqs len {} '.format(n.leng))\n",
    "            decoder_input = n.wordid\n",
    "            decoder_hidden = n.h\n",
    "\n",
    "            if n.wordid.item() == EOS_token and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "#             print(decoder_output.shape)\n",
    "            log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
    "            nextnodes = []\n",
    "\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][new_k].view(-1)\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "\n",
    "#                 print('Previous Node : '.format(n))\n",
    "                node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "\n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "\n",
    "        # choose nbest paths, back trace them\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "        utterance = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance.append(int(n.wordid))\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                utterance.append(int(n.wordid))\n",
    "\n",
    "        utterance = utterance[::-1]\n",
    "#                 utterances.append(utterance)\n",
    "\n",
    "#             decoded_batch.append(utterance)\n",
    "# #             print(decoded_batch)\n",
    "#             decoded_batch = decoded_batch[0][0]\n",
    "        decoded_words = [output_lang.index2word[word] for word in utterance]\n",
    "        return decoded_words, decoder_attentions     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmDv04SW3ssT"
   },
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward  # Length penalty: Keeping it to 0\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.leng < other.leng  \n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.leng > other.leng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "-BTtKHlx3ssV",
    "outputId": "dce69f60-0623-439a-9aee-5ee3e248fc2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 56s (- 83m 14s) (5000 6%) 3.0764\n",
      "11m 44s (- 76m 18s) (10000 13%) 2.5275\n",
      "17m 25s (- 69m 40s) (15000 20%) 2.1858\n",
      "23m 13s (- 63m 53s) (20000 26%) 1.9960\n",
      "29m 0s (- 58m 0s) (25000 33%) 1.7769\n",
      "34m 49s (- 52m 13s) (30000 40%) 1.6092\n",
      "40m 27s (- 46m 14s) (35000 46%) 1.4996\n",
      "46m 5s (- 40m 19s) (40000 53%) 1.4095\n",
      "51m 47s (- 34m 31s) (45000 60%) 1.3005\n",
      "57m 15s (- 28m 37s) (50000 66%) 1.1997\n",
      "62m 42s (- 22m 48s) (55000 73%) 1.1208\n",
      "68m 21s (- 17m 5s) (60000 80%) 1.0423\n",
      "74m 2s (- 11m 23s) (65000 86%) 0.9866\n",
      "79m 48s (- 5m 42s) (70000 93%) 0.9745\n",
      "85m 41s (- 0m 0s) (75000 100%) 0.9336\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000) \n",
    "# 2, 37,  3, 19, 18, 10, 11, 22, 38, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4klWiyH3ssX"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8EPZYkF3ssZ"
   },
   "source": [
    "### Beam width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Fdim8KgMNGAc",
    "outputId": "cc0ce516-8ad5-41c8-c451-e272dac11743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
      "\r",
      "\u001b[K     |█████▍                          | 10kB 27.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 20kB 32.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 30kB 35.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 40kB 38.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 51kB 40.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 9.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-1.7.0 sacrebleu-1.4.9\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhsMEbmh3ssa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI4U_ofK3ssb"
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "def bleuScore(test_y_pred, test_y_actuals): #2 lists\n",
    "    bleu = sacrebleu.corpus_bleu(test_y_pred, [test_y_actuals])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c13oGLo13ssd"
   },
   "outputs": [],
   "source": [
    "def readLangs_test(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    x_files = ['IWSLT14.TED.tst2011.en-fr.en.xml', 'IWSLT14.TED.tst2012.en-fr.en.xml']\n",
    "    y_files = ['IWSLT14.TED.tst2011.en-fr.fr.xml', 'IWSLT14.TED.tst2012.en-fr.fr.xml']\n",
    "    eng = []\n",
    "    fr = []\n",
    "\n",
    "    for i,j in zip(x_files, y_files):\n",
    "        tree_eng = ET.parse(i)\n",
    "        root_eng = tree_eng.getroot()\n",
    "\n",
    "        tree_fr = ET.parse(j)\n",
    "        root_fr = tree_fr.getroot()\n",
    "\n",
    "        for k in root_eng.iter('seg'):\n",
    "            eng.append(k.text.strip())\n",
    "\n",
    "        for l in root_fr.iter('seg'):\n",
    "            fr.append(l.text.strip())\n",
    "            \n",
    "    eng_norm = [normalizeString(l) for l in eng]\n",
    "    fr_norm = [normalizeString(l) for l in fr]\n",
    "    \n",
    "    pairs = zip(eng_norm, fr_norm)\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "klr7awsc3ssf"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly_test(encoder, decoder, n=10):\n",
    "    actual_sen = []\n",
    "    pred_sen = []\n",
    "    for pair in pairs:\n",
    "        actual_sen.append(pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        pred_sen.append(output_sentence)\n",
    "        \n",
    "    return bleuScore(pred_sen, actual_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n2Ke-63T3ssg",
    "outputId": "5c0913f7-ddfc-4c88-8f18-d917dd75fcb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.002932151162845"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5Wcx38L3ssi"
   },
   "source": [
    "### Beam Width = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nbcLGPaa3ssl",
    "outputId": "7d3f0e50-1012-40b7-ff54-de6ba2df0f56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.32002272996931"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWhyZSzH3ssn"
   },
   "source": [
    "### Beam Width = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fLifkL473ssp",
    "outputId": "27cb6805-4a2f-47bb-a08f-18f792db872b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.38203836374787"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BV99SWke3ssr"
   },
   "source": [
    "### Impact on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIwUSGSs3ssr"
   },
   "outputs": [],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLlnfZzf3ssy"
   },
   "source": [
    "### Beam Width = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "quIgrwSV3ss1",
    "outputId": "665b90ec-3cfd-4ec5-c8f1-086d189ce473"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.82248447094242"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0H8vPHlP4_t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_beam_search_mixed_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
