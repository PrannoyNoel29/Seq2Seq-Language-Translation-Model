{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5SOSGIXkll0"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import math\n",
    "import operator\n",
    "import random\n",
    "from queue import PriorityQueue\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0TgizoYkll4"
   },
   "outputs": [],
   "source": [
    " %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ske1mSNykll7"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s29OgScZkll-"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-yl3rtbklmB"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    x_files = ['IWSLT14.TED.dev2010.en-fr.en.xml','IWSLT14.TED.tst2010.en-fr.en.xml']\n",
    "    y_files = ['IWSLT14.TED.dev2010.en-fr.fr.xml','IWSLT14.TED.tst2010.en-fr.fr.xml']\n",
    "    \n",
    "    x_files = ['IWSLT14.TED.tst2011.en-fr.en.xml', 'IWSLT14.TED.tst2012.en-fr.en.xml']\n",
    "    y_files = ['IWSLT14.TED.tst2011.en-fr.fr.xml', 'IWSLT14.TED.tst2012.en-fr.fr.xml']\n",
    "\n",
    "    eng = []\n",
    "    fr = []\n",
    "\n",
    "    for i,j in zip(x_files, y_files):\n",
    "        tree_eng = ET.parse(i)\n",
    "        root_eng = tree_eng.getroot()\n",
    "\n",
    "        tree_fr = ET.parse(j)\n",
    "        root_fr = tree_fr.getroot()\n",
    "\n",
    "        for k in root_eng.iter('seg'):\n",
    "            eng.append(k.text.strip())\n",
    "\n",
    "        for l in root_fr.iter('seg'):\n",
    "            fr.append(l.text.strip())\n",
    "            \n",
    "    eng_norm = [normalizeString(l) for l in eng]\n",
    "    fr_norm = [normalizeString(l) for l in fr]\n",
    "    \n",
    "    pairs = zip(eng_norm, fr_norm)\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1sJBhtVklmD"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "mflN4h4NklmG",
    "outputId": "98d72276-ce5a-4286-81b5-8642317ef0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 1942 sentence pairs\n",
      "Trimmed to 59 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 374\n",
      "eng 355\n",
      "['je vais decouper ici .', 'i m going to make a cut right there .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYWZPm6IklmK"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w2EIHK6qklmM"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, n_layers = 1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, n_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNvwfPA2klmP"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers = 1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers  # 1\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZjE_tHGklmR"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ea6-7oSEklmW"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-u7Zgm-jklmZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTXjgDCMklmc"
   },
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            loss_values.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvpKLRUkklme"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpzswZP1klmg"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "#         decoder_input = torch.tensor([[SOS_token]])  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "#         beam_width = 3\n",
    "        beam_width = 60\n",
    "        topk = 1  # how many sentence do you want to generate\n",
    "        decoded_batch = []\n",
    "\n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([SOS_token])\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "        number_required = 1\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "        nodes = PriorityQueue()\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        qsize = 1\n",
    "\n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 2000: \n",
    "                break\n",
    "#             print(nodes.get())\n",
    "            # fetch the best node\n",
    "            score, n = nodes.get()\n",
    "            \n",
    "            # print('--best node seqs len {} '.format(n.leng))\n",
    "            decoder_input = n.wordid\n",
    "            decoder_hidden = n.h\n",
    "\n",
    "            if n.wordid.item() == EOS_token and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "#             print(decoder_output.shape)\n",
    "            log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
    "            nextnodes = []\n",
    "\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][new_k].view(-1)\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "\n",
    "#                 print('Previous Node : '.format(n))\n",
    "                node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "\n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "\n",
    "        # choose nbest paths, back trace them\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "        utterance = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance.append(int(n.wordid))\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                utterance.append(int(n.wordid))\n",
    "\n",
    "        utterance = utterance[::-1]\n",
    "#                 utterances.append(utterance)\n",
    "\n",
    "#             decoded_batch.append(utterance)\n",
    "# #             print(decoded_batch)\n",
    "#             decoded_batch = decoded_batch[0][0]\n",
    "        decoded_words = [output_lang.index2word[word] for word in utterance]\n",
    "        return decoded_words, decoder_attentions     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EsjMT2Rklmi"
   },
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward  # Length Penalty: keeping it to 0\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.leng < other.leng  \n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.leng > other.leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "dEonFc_Qklml",
    "outputId": "53a8e33f-e1be-474f-e5c3-6f1ea81727bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4m 53s (- 68m 31s) (5000 6%) 1.0494\n",
      "10m 4s (- 65m 27s) (10000 13%) 0.0074\n",
      "15m 16s (- 61m 4s) (15000 20%) 0.0033\n",
      "20m 33s (- 56m 30s) (20000 26%) 0.0021\n",
      "25m 37s (- 51m 14s) (25000 33%) 0.0015\n",
      "30m 55s (- 46m 22s) (30000 40%) 0.0012\n",
      "36m 0s (- 41m 9s) (35000 46%) 0.0010\n",
      "41m 3s (- 35m 55s) (40000 53%) 0.0008\n",
      "46m 3s (- 30m 42s) (45000 60%) 0.0007\n",
      "51m 5s (- 25m 32s) (50000 66%) 0.0007\n",
      "55m 57s (- 20m 21s) (55000 73%) 0.0006\n",
      "60m 53s (- 15m 13s) (60000 80%) 0.0005\n",
      "65m 55s (- 10m 8s) (65000 86%) 0.0005\n",
      "70m 49s (- 5m 3s) (70000 93%) 0.0004\n",
      "75m 46s (- 0m 0s) (75000 100%) 0.0004\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000) \n",
    "# 2, 37,  3, 19, 18, 10, 11, 22, 38, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0mCp-v0klmo"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyAGb0Xxklmq"
   },
   "source": [
    "### Beam width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "wzX5IxhGklmq",
    "outputId": "d0876d53-405f-4d03-81a3-4f67e4575eb4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis celui qui ne veut pas etre interrompu .\n",
      "= i m the one who doesn t want to be interrupted .\n",
      "< SOS i m the one who doesn t want to be interrupted . EOS\n",
      "\n",
      "> je vais vous montrer .\n",
      "= i m going to do some demonstrations .\n",
      "< SOS i m going to do some demonstrations . EOS\n",
      "\n",
      "> je vais faire une biennale internationale j ai besoin d artistes du monde entier .\n",
      "= i m going to do an international biennial i need artists from all around the world .\n",
      "< SOS i m going to do an international biennial i need artists from all around the world . EOS\n",
      "\n",
      "> je suis sous l eau dans la rivere avec ces poissons .\n",
      "= i m under the river with those fish .\n",
      "< SOS i m under the river with those fish . EOS\n",
      "\n",
      "> je suis tellement gene que ca ait coute si peu d obtenir que mon idee me soit implantee que je ne suis pas pret a vous dire ce que ca a coute .\n",
      "= i m so embarrassed at how cheap it was to get from my idea to me being implanted that i m not prepared to tell you what it cost .\n",
      "< SOS i m so embarrassed at how cheap it was to get from my idea to me being implanted that i m not prepared to tell you what it cost . EOS\n",
      "\n",
      "> je suis sur que tout le monde dans cette salle a eu affaire a l arrogance du corps medical des medecins des chirurgiens a un moment ou un autre .\n",
      "= i m sure everyone in this room has come across arrogance amongst medics doctors surgeons at some point .\n",
      "< SOS i m sure everyone in this room has come across arrogance amongst medics doctors surgeons at some point . EOS\n",
      "\n",
      "> je suis un artiste contemporain et je viens d un contexte inattendu .\n",
      "= i m a contemporary artist with a bit of an unexpected background .\n",
      "< SOS i m a contemporary artist with a bit of an unexpected background . EOS\n",
      "\n",
      "> nous sommes subjugues par la technologie .\n",
      "= we re smitten with technology .\n",
      "< SOS we re smitten with technology . EOS\n",
      "\n",
      "> vous etes pousses a faire quelque chose que tout le monde a juge impossible .\n",
      "= you re driven to do something that everyone has told you is impossible .\n",
      "< SOS you re driven to do something that everyone has told you is impossible . EOS\n",
      "\n",
      "> nous sommes tous des specialistes maintenant meme les medecins de premiers secours .\n",
      "= we re all specialists now even the primary care physicians .\n",
      "< SOS we re all specialists now even the primary care physicians . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "wBdf7adM65Pe",
    "outputId": "0aefa613-0c1b-45b1-f945-5757686417e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
      "\r",
      "\u001b[K     |█████▍                          | 10kB 27.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 20kB 32.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 30kB 37.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 40kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 51kB 32.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-1.7.0 sacrebleu-1.4.9\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVIZpTX2klms"
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "def bleuScore(test_y_pred, test_y_actuals): #2 lists\n",
    "    bleu = sacrebleu.corpus_bleu(test_y_pred, [test_y_actuals])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8k1wVeJAklmu"
   },
   "outputs": [],
   "source": [
    "def readLangs_test(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    x_files = ['IWSLT14.TED.tst2011.en-fr.en.xml', 'IWSLT14.TED.tst2012.en-fr.en.xml']\n",
    "    y_files = ['IWSLT14.TED.tst2011.en-fr.fr.xml', 'IWSLT14.TED.tst2012.en-fr.fr.xml']\n",
    "    eng = []\n",
    "    fr = []\n",
    "\n",
    "    for i,j in zip(x_files, y_files):\n",
    "        tree_eng = ET.parse(i)\n",
    "        root_eng = tree_eng.getroot()\n",
    "\n",
    "        tree_fr = ET.parse(j)\n",
    "        root_fr = tree_fr.getroot()\n",
    "\n",
    "        for k in root_eng.iter('seg'):\n",
    "            eng.append(k.text.strip())\n",
    "\n",
    "        for l in root_fr.iter('seg'):\n",
    "            fr.append(l.text.strip())\n",
    "            \n",
    "    eng_norm = [normalizeString(l) for l in eng]\n",
    "    fr_norm = [normalizeString(l) for l in fr]\n",
    "    \n",
    "    pairs = zip(eng_norm, fr_norm)\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7S2ax65klmx"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly_test(encoder, decoder, n=10):\n",
    "    actual_sen = []\n",
    "    pred_sen = []\n",
    "    for pair in pairs:\n",
    "        actual_sen.append(pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        pred_sen.append(output_sentence)\n",
    "        \n",
    "    return bleuScore(pred_sen, actual_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x8ZQkPwAklmz",
    "outputId": "973148ac-ca0f-47f9-ca53-a55837049382"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.96629409321952"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MF71SMHklm1"
   },
   "source": [
    "### Beam Width = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "7ewB8jIXklm2",
    "outputId": "dedcc4fe-fb4a-4378-be6b-b513187f4fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> nous commencons a vendre des voitures electriques et c est genial .\n",
      "= we re starting to sell electric cars which is great .\n",
      "< SOS we re starting to sell electric cars which is great . EOS\n",
      "\n",
      "> je suis ingenieur procede .\n",
      "= i m a process engineer .\n",
      "< SOS i m a process engineer . EOS\n",
      "\n",
      "> nous sommes vous etes la ressource la plus sous employee dans le systeme de sante .\n",
      "= we are you are the most underused resource in health care .\n",
      "< SOS we are you are the most underused resource in health care . EOS\n",
      "\n",
      "> il s interesse a la facon dont nous voyons comme un animal combien nous sommes interesses par le mimetisme et le camouflage .\n",
      "= he is interested in how we see as an animal how we are interested in mimicry and camouflage .\n",
      "< SOS he is interested in how we see as an animal how we are interested in mimicry and camouflage . EOS\n",
      "\n",
      "> je vais l organiser et la diriger et l ouvrir au monde .\n",
      "= i m going to organize it and direct it and get it going in the world .\n",
      "< SOS i m going to organize it and direct it and get it going in the world . EOS\n",
      "\n",
      "> je vais plutot vous parler de quelque chose bien plus interessant que les dates de naissances ou les racines cubiques un peu plus profond et un peu plus proche selon moi que le travail .\n",
      "= i m going to talk instead about something far more interesting than dates of birth or cube roots a little deeper and a lot closer to my mind than work .\n",
      "< SOS i m going to talk instead about something far more interesting than dates of birth or cube roots a little deeper and a lot closer to my mind than work . EOS\n",
      "\n",
      "> nous commencons a vendre des voitures electriques et c est genial .\n",
      "= we re starting to sell electric cars which is great .\n",
      "< SOS we re starting to sell electric cars which is great . EOS\n",
      "\n",
      "> je suis une femme qui aime recevoir des textos qui va vous dire que trop de textos peuvent etre un probleme .\n",
      "= i m a woman who loves getting texts who s going to tell you that too many of them can be a problem .\n",
      "< SOS i m a woman who loves getting texts who s going to tell you that too many of them can be a problem . EOS\n",
      "\n",
      "> nous sommes seuls mais nous avons peur de l intimite .\n",
      "= we re lonely but we re afraid of intimacy .\n",
      "< SOS we re lonely but we re afraid of intimacy . EOS\n",
      "\n",
      "> j en suis un exemple extreme .\n",
      "= i m an extreme example of this .\n",
      "< SOS i m an extreme example of this . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-3f6armjklm5",
    "outputId": "e0bcb5bd-4bd8-429c-9f50-f689ace9e251"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.96629409321952"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajsriaf5klm7"
   },
   "source": [
    "### Beam Width = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Df7yrC-bklm8",
    "outputId": "935a6202-9b11-485a-d72d-1c490b7d6038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je vais decouper des veines des arteres oups ! . . .\n",
      "= i m going to cut some veins arteries . oops ! . . .\n",
      "< SOS i m going to cut some veins arteries . oops ! . . . EOS\n",
      "\n",
      "> vous etes pousses a faire quelque chose que tout le monde a juge impossible .\n",
      "= you re driven to do something that everyone has told you is impossible .\n",
      "< SOS you re driven to do something that everyone has told you is impossible . EOS\n",
      "\n",
      "> ils s entrainent juste .\n",
      "= they re just practicing .\n",
      "< SOS they re just practicing . EOS\n",
      "\n",
      "> je suis acteur .\n",
      "= i m a performer .\n",
      "< SOS i m a performer . EOS\n",
      "\n",
      "> je vais me contenter de tout faire moi meme .\n",
      "= i m just going to make the whole thing myself .\n",
      "< SOS i m just going to make the whole thing myself . EOS\n",
      "\n",
      "> nous sommes tous des specialistes maintenant meme les medecins de premiers secours .\n",
      "= we re all specialists now even the primary care physicians .\n",
      "< SOS we re all specialists now even the primary care physicians . EOS\n",
      "\n",
      "> nous sommes subjugues par la technologie .\n",
      "= we re smitten with technology .\n",
      "< SOS we re smitten with technology . EOS\n",
      "\n",
      "> je suis acteur .\n",
      "= i m a performer .\n",
      "< SOS i m a performer . EOS\n",
      "\n",
      "> je suis optimiste .\n",
      "= i m optimistic .\n",
      "< SOS i m optimistic . EOS\n",
      "\n",
      "> nous nous habituons a une nouvelle facon d etre seuls ensemble .\n",
      "= we re getting used to a new way of being alone together .\n",
      "< SOS we re getting used to a new way of being alone together . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EWlsjbULklm-",
    "outputId": "8bfbfdae-1062-4bb2-ce2e-28f9389aa616"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.96629409321952"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HocXuB9QklnA"
   },
   "source": [
    "### Impact on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HqliDEjklnA"
   },
   "outputs": [],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dyZ081v-klnC",
    "outputId": "b5e72c11-0724-4144-e9ea-6fed4f8c8a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = nous sommes optimiste par la chirurgie .\n",
      "output = SOS i m going to do some cuts . EOS\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"nous sommes optimiste par la chirurgie .\") #beam width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SCzwxTtlklnE",
    "outputId": "24575fb0-30e4-4014-9414-64cbfac2fb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = nous sommes optimiste par la chirurgie .\n",
      "output = SOS i m going to do some cuts . EOS\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"nous sommes optimiste par la chirurgie .\") #beam width = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "S4z5hyniklnH",
    "outputId": "0b435693-72c1-4485-e90f-9c2fec5d21db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = nous sommes optimiste par la chirurgie .\n",
      "output = SOS i m going to do some cuts . EOS\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"nous sommes optimiste par la chirurgie .\") #beam width = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35Ag6ZuzklnJ"
   },
   "source": [
    "### Beam Width = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "BWOoBKkOklnJ",
    "outputId": "9adc0d71-3bf6-4f96-c5a9-3a58d34d0cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ils acceptent plus la tristesse que les gens plus jeunes .\n",
      "= they re more accepting of sadness than younger people are .\n",
      "< SOS they re more accepting of sadness than younger people are . EOS\n",
      "\n",
      "> nous sommes subjugues par la technologie .\n",
      "= we re smitten with technology .\n",
      "< SOS we re smitten with technology . EOS\n",
      "\n",
      "> nous poussons l efficience du carburant vers de nouveaux sommets .\n",
      "= we re pushing fuel efficiency to new heights .\n",
      "< SOS we re pushing fuel efficiency to new heights . EOS\n",
      "\n",
      "> c est une artiste suisse tres serieuse .\n",
      "= she s a very serious swiss artist .\n",
      "< SOS she s a very serious swiss artist . EOS\n",
      "\n",
      "> ils sont aussi assez riches et aises et toutes ces autres sortes de choses .\n",
      "= they re also fairly wealthy and affluent and all these other sorts of things .\n",
      "< SOS they re also fairly wealthy and affluent and all these other sorts of things . EOS\n",
      "\n",
      "> ce sont des specimens de papillons qui ont un camouflage naturel .\n",
      "= they re butterfly specimens who have a natural camouflage .\n",
      "< SOS they re butterfly specimens who have a natural camouflage . EOS\n",
      "\n",
      "> ici elle peint une fresque murale de ses horribles dernieres semaines a l hopital .\n",
      "= she s painting here a mural of his horrible final weeks in the hospital .\n",
      "< SOS she s painting here a mural of his horrible final weeks in the hospital . EOS\n",
      "\n",
      "> ce sont des specimens de papillons qui ont un camouflage naturel .\n",
      "= they re butterfly specimens who have a natural camouflage .\n",
      "< SOS they re butterfly specimens who have a natural camouflage . EOS\n",
      "\n",
      "> je suis tellement gene que ca ait coute si peu d obtenir que mon idee me soit implantee que je ne suis pas pret a vous dire ce que ca a coute .\n",
      "= i m so embarrassed at how cheap it was to get from my idea to me being implanted that i m not prepared to tell you what it cost .\n",
      "< SOS i m so embarrassed at how cheap it was to get from my idea to me being implanted that i m not prepared to tell you what it cost . EOS\n",
      "\n",
      "> je vais vous montrer .\n",
      "= i m going to do some demonstrations .\n",
      "< SOS i m going to do some demonstrations . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6PJpimcIklnL",
    "outputId": "ec03b6e2-62e5-4d18-8cd3-1fb539d6b143"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.99946291478088"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateRandomly_test(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eqKc1DoBIsA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq-beam-search_final-length50.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
